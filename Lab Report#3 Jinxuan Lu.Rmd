---
title: 'Lab Report #3'
author: "Jinxuan Lu"
date: "2022/01/14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#loading packages

```{r}
Sys.setenv(LANG="en")
getwd()
library(GGally) # for ggcorr
library(corrr) # network_plot
library(ggcorrplot) # for ggcorrplot
library(FactoMineR) # multiple PCA functions
library(factoextra) # visualisation functions for PCA (e.g. fviz_pca_var) 
library(paran) # for paran

library(psych) # for the mixedCor, cortest.bartlett, KMO, fa functions
library(tidyverse)
library(dplyr)
library(car) # for vif
library(GPArotation) # for the psych fa function to have the required rotation functional 

library(MVN) # for mvn function
library(ICS) # for multivariate skew and kurtosis test library(tidyverse) # for tidy code
library(mice) #for Multivariate Imputation via chained equations
library(apaTables)

#custom function
fviz_loadnings_with_cor <- function(mod, axes = 1, loadings_above = 0.4){	
  require(factoextra)	
  require(dplyr)	
  require(ggplot2)	
	
	
	
if(!is.na(as.character(mod$call$call)[1])){	
  if(as.character(mod$call$call)[1] == "PCA"){	
  contrib_and_cov = as.data.frame(rbind(mod[["var"]][["contrib"]], mod[["var"]][["cor"]]))	
	
vars = rownames(mod[["var"]][["contrib"]])	
attribute_type = rep(c("contribution","correlation"), each = length(vars))	
contrib_and_cov = cbind(contrib_and_cov, attribute_type)	
contrib_and_cov	
	
plot_data = cbind(as.data.frame(cbind(contrib_and_cov[contrib_and_cov[,"attribute_type"] == "contribution",axes], contrib_and_cov[contrib_and_cov[,"attribute_type"] == "correlation",axes])), vars)	
names(plot_data) = c("contribution", "correlation", "vars")	
	
plot_data = plot_data %>% 	
  mutate(correlation = round(correlation, 2))	
	
plot = plot_data %>% 	
  ggplot() +	
  aes(x = reorder(vars, contribution), y = contribution, gradient = correlation, label = correlation)+	
  geom_col(aes(fill = correlation)) +	
  geom_hline(yintercept = mean(plot_data$contribution), col = "red", lty = "dashed") + scale_fill_gradient2() +	
  xlab("variable") +	
  coord_flip() +	
  geom_label(color = "black", fontface = "bold", position = position_dodge(0.5))	
	
	
}	
} else if(!is.na(as.character(mod$Call)[1])){	
  	
  if(as.character(mod$Call)[1] == "fa"){	
    loadings_table = mod$loadings %>% 	
      matrix(ncol = ncol(mod$loadings)) %>% 	
      as_tibble() %>% 	
      mutate(variable = mod$loadings %>% rownames()) %>% 	
      gather(factor, loading, -variable) %>% 	
      mutate(sign = if_else(loading >= 0, "positive", "negative"))	
  	
  if(!is.null(loadings_above)){	
    loadings_table[abs(loadings_table[,"loading"]) < loadings_above,"loading"] = NA	
    loadings_table = loadings_table[!is.na(loadings_table[,"loading"]),]	
  }	
  	
  if(!is.null(axes)){	
  	
  loadings_table = loadings_table %>% 	
     filter(factor == paste0("V",axes))	
  }	
  	
  	
  plot = loadings_table %>% 	
      ggplot() +	
      aes(y = loading %>% abs(), x = reorder(variable, abs(loading)), fill = loading, label =       round(loading, 2)) +	
      geom_col(position = "dodge") +	
      scale_fill_gradient2() +	
      coord_flip() +	
      geom_label(color = "black", fill = "white", fontface = "bold", position = position_dodge(0.5)) +	
      facet_wrap(~factor) +	
      labs(y = "Loading strength", x = "Variable")	
  }	
}	
	

return(plot)	
	
}	
```

#import data & check for descriptive statistics
```{r}
#loading dataset
ARS <- read_csv("https://raw.githubusercontent.com/kekecsz/SIMM61-Course-materials/main/Exercise_06%20-%20CFA%20and%20EFA/animalrights.csv")	
View(ARS)

ARS <- ARS %>% 	
  mutate(sex = factor(sex),
         party = factor(party))

#descriptive statistics
ARS %>% summary()  
ARS %>% describe()
dim(ARS)    #check dimensions
str(ARS)

#checking missing values
sum(is.na(ARS))
colSums(is.na(ARS))
md.pattern(ARS)

#calculating the percentage of missing data
datm <- as.matrix(ARS)
round((sum(is.na(datm)) / length(datm)) * 100, 2)    
#23% missing values, which is kind high (>10%), but they are concentratedly in five rows, so I'll just delete the five rows instead of using imputation method


#remove missing values
ARS <- ARS %>% 	
  drop_na()	

View(ARS)
dim(ARS)

```


#creating a correlation matrix
```{r}
#run a correlation matrix of all predictors (Pearson Correlation)
ARS_items_only = ARS %>% select(starts_with('ar'))	
View(ARS_items_only)
head(ARS_items_only)
rowSums(head(ARS_items_only))   #Take a look at the first few lines of the response data and their corresponding sum scores

ARS_correl = ARS_items_only %>% 	
  cor(use = "pairwise")	
str(ARS_correl)   
lowerMat(ARS_correl)

#table including means, standard deviation, and correlation among 28 items
apa.cor.table(ARS_items_only,
              show.conf.interval = TRUE,
              show.sig.stars = TRUE,
              landscape = TRUE)

#test correlations' significance -- p values and confidence intervals
corr.test(ARS_items_only, use = "pairwise.complete.obs")$p
corr.test(ARS_items_only, use = "pairwise.complete.obs")$ci

# Graphical representation of error
error.dots(ARS_items_only)
error.bars(ARS_items_only)
```

#check & deal with outliers
```{r}
#Checking multivariate outliers in correlation 
outLiers <- psych::outlier(ARS_items_only)
outLiers

alph_crit <- .001
n_nu <- ncol(ARS_items_only)
crit_val <- (qchisq(p = 1-(alph_crit), df = n_nu))
crit_val

outers <- data.frame(outLiers) %>%
            filter(outLiers > crit_val) %>%
            arrange(desc(outLiers))
outers     #it suggests that the 35th, 93th, 104th, 114th, and 128th are outliers

 
#create another dataset without outliers
ARS_without_outliers <- ARS_items_only %>% slice(-c(35, 93, 104, 114, 128))
View(ARS_without_outliers)

ARS_correl_outliers = ARS_without_outliers %>% 	
  cor(use = "pairwise")	
str(ARS_correl_outliers) 
lowerMat(ARS_correl_outliers)
```

#sensitive analysis by comparing with the one without outliers
```{r}
#visualizing the correlation structure
ggcorr(ARS_correl)
ggcorr(ARS_correl_outliers)
	
ggcorrplot(cor(ARS_items_only), p.mat = cor_pmat(ARS_items_only), hc.order=TRUE, type='lower')
ggcorrplot(cor(ARS_without_outliers), p.mat = cor_pmat(ARS_without_outliers), hc.order=TRUE, type='lower')

cor(ARS_items_only) %>% network_plot(min_cor=0.6)	
cor(ARS_without_outliers) %>% network_plot(min_cor=0.6)	

```


#checking assumption of reliability & factorability
```{r}
#Reliability check: alpha() -- Cronbach's alpha, it is a measure of the internal consistency of my measure, usually > .8
alpha(ARS_items_only, check.keys = TRUE)  #alpha = 0.92, good reliability
alpha(ARS_without_outliers, check.keys = TRUE)  #alpha = 0.91

#Factorability check
##Barlett sphericity test
bfi_factorability_ARS <- cortest.bartlett(ARS_correl)	
bfi_factorability_ARS	  #p value <0.05, the bfi assumption is rejected, so the original variables correlate with each other, EFA can be conducted

bfi_factorability_ARS_outliers <- cortest.bartlett(ARS_correl_outliers)	
bfi_factorability_ARS_outliers  #p value < 0.05

##Kaiser-Meyer-Olkin (KMO) test
KMO(ARS_correl)	
# the KMO is higher than 0.6 (0.87) in all cases, and the total KMO is also higher than 0.6**, so the data seems to be factorable.	

KMO(ARS_correl_outliers)	  #0.86

det(ARS_correl)  #a positive determinant, which means the factor analysis will probably run
det(ARS_correl_outliers)  #positive

```


#Factor extraction (using function **fa()**)
```{r}
#determine whether the data show a multivariate normal distribution. 
result <- mvn(ARS[,1:28], mvnTest = "hz")	
result$multivariateNormality	  #p-value = 0

result_outliers <- mvn(ARS_without_outliers, mvnTest = "hz")	
result_outliers$multivariateNormality   #p-value = 0
	
mvnorm.kur.test(na.omit(ARS[,1:28]))	 #p-value < 0.05
mvnorm.skew.test(na.omit(ARS[,1:28]))	 #p-value < 0.05
#the p-values of the Henze-Zirkler test & the multivariate skewedness and kurtosis tests are all lower than 0.05, indicating violation of the multivariate normality assumption.	

mvnorm.kur.test(na.omit(ARS_without_outliers))	 #p-value < 0.05
mvnorm.skew.test(na.omit(ARS_without_outliers))	 #p-value < 0.05

```

#determining how many factors to retain
```{r}
#check out the scree test and the Kaiser-Guttman criterion
scree(ARS_correl)
scree(ARS_correl_outliers)

#use parallel analysis for estimation
fa.parallel(ARS_correl, n.obs = nrow(ARS), fa = "both", n.iter = 100, fm = "pa")  

fa.parallel(ARS_correl_outliers, n.obs = nrow(ARS_without_outliers), fa = "both", n.iter = 100, fm = "pa")  
#parallel analysis suggests that the number of factors = 4 and the number of components = 2


#The Very Simple Structure (VSS) criterion and Wayne Velicer's Minimum Average Partial (MAP) criterion
nfactors(ARS_correl, n.obs = nrow(ARS))	  #VSS and MAP both suggest 2 factors
nfactors(ARS_correl_outliers, n.obs = nrow(ARS_without_outliers))	  #VSS and MAP both suggest 2 factors

# - Scree test: 2
# - Parallel analysis: 4
# - VSS: 2	
# - MAP: 2	
```


#Since most tests suggest 2 factors, I'll follow the suggestion

#The 2-factor structure analysis:
```{r}
#Factor extraction without rotation
ARS_mod1 <- fa(ARS_correl, nfactors = 2, rotate = "none", fm = "pa")
ARS_mod1     #2-factor structure explained 34% variance of 28 items

ARS_mod1_outliers <- fa(ARS_correl_outliers, nfactors = 2, rotate = "none", fm = "pa")
ARS_mod1_outliers    #2-factor structure explained 35% variance of 28 items

# Sorted communality 
ARS_mod1_common <- as.data.frame(sort(ARS_mod1$communality, decreasing = TRUE))	
ARS_mod1_common 	#ar6 is the best represented item in the 2-factor structure, with 67% of its total variance explained by the new factors.

ARS_mod1_common_outliers <- as.data.frame(sort(ARS_mod1_outliers$communality, decreasing = TRUE))	
ARS_mod1_common_outliers  #ar13, 67%

mean(ARS_mod1$communality)	   #0.343
mean(ARS_mod1_outliers$communality)  #0.346
```

#rotation
```{r}
#Factor extraction with rotation 
ARS_mod1_promax <- fa(ARS_correl, nfactors = 2, rotate = "promax", fm = "pa")
ARS_mod1_promax     #2-factor structure explained 34% variance of 28 items
#the lowest correlation between factors is 0.7, which exceeds the Tabachnick and Fiddell threshold of .32

ARS_mod1_promax_outliers <- fa(ARS_correl_outliers, nfactors = 2, rotate = "promax", fm = "pa")
ARS_mod1_promax_outliers   #35%  0.67

# Sorted communality 
ARS_mod1_promax_common <- as.data.frame(sort(ARS_mod1_promax$communality, decreasing = TRUE))	
ARS_mod1_promax_common 	#a6 67%	

ARS_mod1_promax_common_outliers <- as.data.frame(sort(ARS_mod1_promax_outliers$communality, decreasing = TRUE))	
ARS_mod1_promax_common_outliers  #ar13  67%

mean(ARS_mod1_promax$communality)	   #0.34
mean(ARS_mod1_promax_outliers$communality)   #0.35

ARS_mod1_promax_uniq <- as.data.frame(sort(ARS_mod1_promax$uniquenesses, decreasing = TRUE))
ARS_mod1_promax_uniq  #the part that cannot be explained by factors

ARS_mod1_promax$loadings
print(ARS_mod1_promax$loadings, cutoff = 0.32)
#The value 0.32 is handy because we can interpret its squared value, 0.1024, as the minimum proportion of variance (i.e., 10%) in the observed variable (subtests) we wish to consider to be salient enough for consideration (Tabachnick & Fidell, 2007).

ARS_mod1_promax_outliers$loadings
print(ARS_mod1_promax_outliers$loadings, cutoff = 0.32)
```

#visualization
```{r}
#visualize the rotation result
factor.plot(ARS_mod1_promax)
factor.plot(ARS_mod1_promax_outliers)

fa.diagram(ARS_mod1_promax)	
fa.diagram(ARS_mod1_promax, simple = FALSE)

fa.diagram(ARS_mod1_promax_outliers)	
fa.diagram(ARS_mod1_promax_outliers, simple = FALSE)
	
fviz_loadnings_with_cor(ARS_mod1_promax, axes = 1, loadings_above = 0.32)	 
fviz_loadnings_with_cor(ARS_mod1_promax_outliers, axes = 1, loadings_above = 0.32)
	
fviz_loadnings_with_cor(ARS_mod1_promax, axes = 2, loadings_above = 0.32)	
fviz_loadnings_with_cor(ARS_mod1_promax_outliers, axes = 2, loadings_above = 0.32)	

```


#According to the results and the theory, 2 Factors are chosen to keep in the final factor structure
#The loadings of factors in the no-outliers-model do not make sense for some of the items based on theory, so I'll keep the original dataset

#items with very low loadings on both factors: ar1, ar8, ar11, ar14, ar25; and ar15 has cross loadings
#remove ar1, ar8, ar11, ar14, ar25 first

```{r}
#creat new data and check data
ARS_items_only = ARS %>% 	
                  select(ar1:ar28)	
ARS_items_only_rm = ARS_items_only[, -c(1,8,11,14,25)]
View(ARS_items_only_rm)

head(ARS_items_only_rm)
rowSums(head(ARS_items_only_rm))  

ARS_correl_rm = ARS_items_only_rm %>% 	
  cor()	
str(ARS_correl_rm)
round(ARS_correl_rm, 3)      

#table including means, standard deviation, and correlation among 23 items
apa.cor.table(ARS_items_only_rm,
              show.conf.interval = TRUE,
              show.sig.stars = TRUE,
              landscape = TRUE)
```

```{r}
#visualizing the correlation structure
ggcorr(ARS_correl_rm)	
	
ggcorrplot(cor(ARS_items_only_rm), p.mat = cor_pmat(ARS_items_only_rm), hc.order=TRUE, type='lower')
```

#checking assumption of reliability & factorability
```{r}
#Reliability check: alpha() -- Cronbach's alpha
alpha(ARS_items_only_rm, check.keys = TRUE)  #alpha = 0.91, good reliability

#Factorability check
##Barlett sphericity test
bfi_factorability_ARS_rm <- cortest.bartlett(ARS_correl_rm)	
bfi_factorability_ARS_rm	  #p value <0.05, the bfi assumption is rejected

##Kaiser-Meyer-Olkin (KMO) test
KMO(ARS_correl_rm)	
# the KMO is higher than 0.6 (0.88) in all cases, and the total KMO is also higher than 0.6**, so the data seems to be factorable.	

det(ARS_correl_rm)  #a positive determinant, which means the factor analysis will probably run
```

#Factor extraction (using function **fa()**)
```{r}
#determine whether the data show a multivariate normal distribution. 
result_rm <- mvn(ARS_items_only_rm, mvnTest = "hz")	
result_rm$multivariateNormality	  #p-value = 0
	
mvnorm.kur.test(na.omit(ARS_items_only_rm))	 #p-value < 0.05
mvnorm.skew.test(na.omit(ARS_items_only_rm))	 #p-value < 0.05
#indicating violation of the multivariate normality assumption.	
```

#determining how many factors to retain first
```{r}
#check out the scree test and the Kaiser-Guttman criterion
scree(ARS_correl_rm)

#use parallel analysis for estimation with the paf extraction method
fa.parallel(ARS_correl_rm, n.obs = nrow(ARS), fa = "both", n.iter = 100, fm = "pa")  
#parallel analysis suggests that the number of factors = 3 and the number of components = 2

#The Very Simple Structure (VSS) criterion and Wayne Velicer's Minimum Average Partial (MAP) criterion
nfactors(ARS_correl_rm, n.obs = nrow(ARS))	  #VSS and MAP both suggest 2 factors

# - Scree test: 2
# - Parallel analysis: 3
# - VSS: 2	
# - MAP: 2	
```


#Since I've decided 2-factor structure, and most tests suggest 2-factor structure, I'll stick with 2
```{r}
#Factor extraction without rotation
ARS_mod2 <- fa(ARS_correl_rm, nfactors = 2, rotate = "none", fm = "pa")
ARS_mod2     #2-factor structure explained 37% variance of 23 items

# Sorted communality	
ARS_mod2_common <- as.data.frame(sort(ARS_mod2$communality, decreasing = TRUE))	
ARS_mod2_common 	#ar6   69% 
	
mean(ARS_mod2$communality)	#0.371

```

#rotation
```{r}
#promax
ARS_mod2_promax <- fa(ARS_correl_rm, nfactors = 2, rotate = "promax", fm = "pa")
ARS_mod2_promax     #37% variance of 23 items

#try varimax
ARS_mod2_varimax <- fa(ARS_correl_rm, nfactors = 2, rotate = "varimax", fm = "pa")
ARS_mod2_varimax   #37%

# Sorted communality
ARS_mod2_promax_common <- as.data.frame(sort(ARS_mod2_promax$communality, decreasing = TRUE))	
ARS_mod2_promax_common 	#a6  69%

ARS_mod2_varimax_common <- as.data.frame(sort(ARS_mod2_varimax$communality, decreasing = TRUE))	
ARS_mod2_varimax_common   #ar6   69%
	
mean(ARS_mod2_promax$communality)	   #0.371
mean(ARS_mod2_varimax$communality)	   #0.371

ARS_mod2_promax$loadings
print(ARS_mod2_promax$loadings, cutoff = 0.32)   #ar15 still has cross loadings

ARS_mod2_varimax$loadings
print(ARS_mod2_varimax$loadings, cutoff = 0.32)    #too many cross loadings, so I'll give up varimax rotation

```

```{r}
#visualize the rotation result
factor.plot(ARS_mod2_promax)

fa.diagram(ARS_mod2_promax)	
	
fviz_loadnings_with_cor(ARS_mod2_promax, axes = 1, loadings_above = 0.32)	
	
fviz_loadnings_with_cor(ARS_mod2_promax, axes = 2, loadings_above = 0.32)	

#since ar15 still has cross loading, so I make the loading cutoff a little bit higher
fviz_loadnings_with_cor(ARS_mod2_promax, axes = 1, loadings_above = 0.35)	
	
fviz_loadnings_with_cor(ARS_mod2_promax, axes = 2, loadings_above = 0.35)	
#it seems that ar16 was excluded

```

#based on this result, ar16 should be removed, and based on theory, the question is indeed too general

#remove more items to achieve simple structure
```{r}
#creat new data and check data
ARS_items_only = ARS %>% 	
                  select(ar1:ar28)	
ARS_items_only_rm1 = ARS_items_only[, -c(1,8,11,14,16,25)]
View(ARS_items_only_rm1)

#descriptive statistics
ARS_items_only_rm1 %>% summary()  
ARS_items_only_rm1 %>% describe()
dim(ARS_items_only_rm1)    #check dimensions
str(ARS_items_only_rm1)

ARS_correl_rm1 = ARS_items_only_rm1 %>% 	
  cor()	
str(ARS_correl_rm1)
round(ARS_correl_rm1, 3)       #Pearson Correlation of the ARS data: most items have some correlation with each other

#table including means, standard deviation, and correlation among 22 items
apa.cor.table(ARS_items_only_rm1,
              show.conf.interval = TRUE,
              show.sig.stars = TRUE,
              landscape = TRUE)
```

```{r}
#visualizing the correlation structure
ggcorr(ARS_correl_rm1)	
	
ggcorrplot(cor(ARS_items_only_rm1), p.mat = cor_pmat(ARS_items_only_rm1), hc.order=TRUE, type='lower')
```

#checking assumption of reliability & factorability
```{r}
#Reliability check: alpha() -- Cronbach's alpha
alpha(ARS_items_only_rm1, check.keys = TRUE)  #alpha = 0.9, good reliability

#Factorability check
##Barlett sphericity test
bfi_factorability_ARS_rm1 <- cortest.bartlett(ARS_correl_rm1)	
bfi_factorability_ARS_rm1	  #p value = <0.05, the bfi assumption is rejected

##Kaiser-Meyer-Olkin (KMO) test
KMO(ARS_correl_rm1)	
# the KMO is higher than 0.6 (0.87) in all cases, and the total KMO is also higher than 0.6**, so the data seems to be factorable.	

det(ARS_correl_rm1)  #a positive determinant means the factor analysis will probably run

```

#Factor extraction (using function **fa()**)
```{r}
#determine whether the data show a multivariate normal distribution. 
result_rm1 <- mvn(ARS_items_only_rm1, mvnTest = "hz")	
result_rm1$multivariateNormality	  #p-value = 0
	
mvnorm.kur.test(na.omit(ARS_items_only_rm1))	 #p-value < 0.05
mvnorm.skew.test(na.omit(ARS_items_only_rm1))	 #p-value < 0.05
#indicating violation of the multivariate normality assumption
```

#determining how many factors to retain first
```{r}
#check out the scree test and the Kaiser-Guttman criterion
scree(ARS_correl_rm1)

#use parallel analysis for estimation with the paf extraction method
fa.parallel(ARS_correl_rm1, n.obs = nrow(ARS), fa = "both", n.iter = 100, fm = "pa")  
#parallel analysis suggests that the number of factors = 3 and the number of components = 2

#The Very Simple Structure (VSS) criterion and Wayne Velicer's Minimum Average Partial (MAP) criterion
nfactors(ARS_correl_rm1, n.obs = nrow(ARS))	  #VSS and MAP both suggest 2 factors

# - Scree test: 2
# - Parallel analysis: 3
# - VSS: 2	
# - MAP: 2	
```

#this time, I'll try both 3 and 2 factors
#try 3-factor structure with 22 items
```{r}
#Factor extraction without rotation
ARS_mod3 <- fa(ARS_correl_rm1, nfactors = 3, rotate = "none", fm = "pa")
ARS_mod3     #the result shows that 3 factors explained 42% variance of 22 items

# Sorted communality (which is h^2 -- common variance)	
ARS_mod3_common <- as.data.frame(sort(ARS_mod3$communality, decreasing = TRUE))	
ARS_mod3_common 	# ar5   83%
	
mean(ARS_mod3$communality)	#0.424
```

```{r}
#Factor extraction with rotation 
ARS_mod3_promax <- fa(ARS_correl_rm1, nfactors = 3, rotate = "promax", fm = "pa")
ARS_mod3_promax     # 42%

# Sorted communality 
ARS_mod3_promax_common <- as.data.frame(sort(ARS_mod3_promax$communality, decreasing = TRUE))	
ARS_mod3_promax_common 	#ar5 83%	
	
mean(ARS_mod3_promax$communality)	   #0.424

ARS_mod3_promax$loadings
print(ARS_mod3_promax$loadings, cutoff = 0.32)   #ar10 has cross loadings, and ar3 has very low loadings
```

```{r}
#visualize the rotation result
factor.plot(ARS_mod3_promax)

fa.diagram(ARS_mod3_promax)	
	
fviz_loadnings_with_cor(ARS_mod3_promax, axes = 1, loadings_above = 0.35)	   
	
fviz_loadnings_with_cor(ARS_mod3_promax, axes = 2, loadings_above = 0.35)	 

fviz_loadnings_with_cor(ARS_mod3_promax, axes = 3, loadings_above = 0.35)	

```


#keep sticking with the 2-factor model
```{r}
#Factor extraction without rotation
ARS_mod4 <- fa(ARS_correl_rm1, nfactors = 2, rotate = "none", fm = "pa")
ARS_mod4     #the result shows that 2 factors explained 38% variance of 22 items

# Sorted communality (which is h^2 -- common variance)	
ARS_mod4_common <- as.data.frame(sort(ARS_mod4$communality, decreasing = TRUE))	
ARS_mod4_common 	#the communalities indicate the proportion of variance in the observed variable explained by the extracted factors. The output shows that ar6 is the best represented item in the 2-factor structure, with 69% of its total variance explained by the new factors.	
	
mean(ARS_mod4$communality)	#0.380

```

#rotation
```{r}
#Factor extraction with rotation 
ARS_mod4_promax <- fa(ARS_correl_rm1, nfactors = 2, rotate = "promax", fm = "pa")
ARS_mod4_promax     #the result shows that 2 factors explained 38% variance of 22 items
#And the resulting correlation matrix for the factors shows that even the lowest correlation is 0.65, which exceeds the Tabachnick and Fiddell threshold of .32

# Sorted communality (which is h^2 -- common variance)	
ARS_mod4_promax_common <- as.data.frame(sort(ARS_mod4_promax$communality, decreasing = TRUE))	
ARS_mod4_promax_common 	#the output shows that ar6 is the best represented item in the 2-factor structure, with 69% of its total variance explained by the new factors.	
	
mean(ARS_mod4_promax$communality)	   #0.380

ARS_mod4_promax$loadings
print(ARS_mod4_promax$loadings, cutoff = 0.32)   #ar15 has cross loadings

#increase cutoff
print(ARS_mod4_promax$loadings, cutoff = 0.36, sort = TRUE)   #perfect

```

```{r}
#visualize the rotation result
factor.plot(ARS_mod4_promax)

fa.diagram(ARS_mod4_promax)	
	
fviz_loadnings_with_cor(ARS_mod4_promax, axes = 1, loadings_above = 0.36)	   #concerns about animal research
fviz_loadnings_with_cor(ARS_mod4_promax, axes = 2, loadings_above = 0.36)	   #concerns about general animal rights

```

#getting final results as tables
```{r}
library(gt)

fa_table <- function(x, varlabels = NULL, title = "Factor analysis results", diffuse = .10, small = .35, cross = .20, sort = TRUE) {
  #get sorted loadings
  require(dplyr)
  require(purrr)
  require(tibble)
  require(gt)
  if(sort == TRUE) {
    x <- psych::fa.sort(x)
  }
  if(!is.null(varlabels)) {
    if(length(varlabels) != nrow(x$loadings)) { warning("Number of variable labels and number of variables are unequal. Check your input!",
                                                        call. = FALSE) }
    if(sort == TRUE) {
      varlabels <- varlabels[x$order]
      }
  }
  if(is.null(varlabels)) {varlabels <- rownames(x$loadings)}

  loadings <- data.frame(unclass(x$loadings))
  
  #make nice names
  factornamer <- function(nfactors) {
    paste0("Factor_", 1:nfactors)}
  
  nfactors <- ncol(loadings)
  fnames <- factornamer(nfactors)
  names(loadings) <- fnames
  
  # prepare locations
  factorindex <- apply(loadings, 1, function(x) which.max(abs(x)))
  
  # adapted from sjplot: getremovableitems
  getRemovableItems <- function(dataframe, fctr.load.tlrn = diffuse) {
    # clear vector
    removers <- vector(length = nrow(dataframe))
    # iterate each row of the data frame. each row represents
    # one item with its factor loadings
    for (i in seq_along(removers)) {
      # get factor loadings for each item
      rowval <- as.numeric(abs(dataframe[i, ]))
      # retrieve highest loading
      maxload <- max(rowval)
      # retrieve 2. highest loading
      max2load <- sort(rowval, TRUE)[2]
      # check difference between both
      if (abs(maxload - max2load) < fctr.load.tlrn) {
        # if difference is below the tolerance,
        # remeber row-ID so we can remove that items
        # for further PCA with updated data frame
        removers[i] <- TRUE
      }
    }
    # return a vector with index numbers indicating which items
    # have unclear loadings
    return(removers)
  }
 if(nfactors > 1) {
   removable <- getRemovableItems(loadings)
   cross_loadings <- purrr::map2(fnames, seq_along(fnames), function(f, i) {
     (abs(loadings[,f] > cross)) & (factorindex != i) 
   })
 }

  small_loadings <- purrr::map(fnames, function(f) {
    abs(loadings[,f]) < small
  })
  
  ind_table <- dplyr::tibble(varlabels, loadings) %>%
    dplyr::rename(Indicator = varlabels) %>% 
    dplyr::mutate(Communality = x$communality, Uniqueness = x$uniquenesses, Complexity = x$complexity) %>% 
    dplyr::mutate(across(starts_with("Factor"), round, 3))  %>%
    dplyr::mutate(across(c(Communality, Uniqueness, Complexity), round, 2))
                    
  
  ind_table <- ind_table %>% gt(rowname_col = "Indicator") %>% tab_header(title = title)
  # mark small loadiongs
  for(f in seq_along(fnames)) {
    ind_table <- ind_table %>%  tab_style(style = cell_text(color = "#D3D3D3", style = "italic"),
                             locations = cells_body(columns = fnames[f], rows = small_loadings[[f]]))
  }
  # mark cross loadings
  
  if (nfactors > 1) {
    for (f in seq_along(fnames)) {
      ind_table <-
        ind_table %>%  tab_style(
          style = cell_text(style = "italic"),
          locations = cells_body(columns = fnames[f], rows = cross_loadings[[f]])
        )
    }
    # mark non-assignable indicators
    ind_table <-
      ind_table %>%  tab_style(style = cell_fill(color = "#D93B3B"),
                               locations = cells_body(rows = removable))
  }
  
  # adapted from https://www.anthonyschmidt.co/post/2020-09-27-efa-tables-in-r/
  Vaccounted <- x[["Vaccounted"]]
  colnames(Vaccounted) <- fnames 
  if (nfactors > 1) {
  Phi <- x[["Phi"]]
  rownames(Phi) <- fnames
  colnames(Phi) <- fnames
  f_table <- rbind(Vaccounted, Phi) %>%
    as.data.frame() %>% 
    rownames_to_column("Property") %>%
    mutate(across(where(is.numeric), round, 3)) %>%
    gt() %>% tab_header(title = "Eigenvalues, Variance Explained, and Factor Correlations for Rotated Factor Solution")
  }
  else if(nfactors == 1) {
    f_table <- rbind(Vaccounted) %>%
      as.data.frame() %>% 
      rownames_to_column("Property") %>%
      mutate(across(where(is.numeric), round, 3)) %>%
      gt() %>% tab_header(title = "Eigenvalues, Variance Explained, and Factor Correlations for Rotated Factor Solution")
  }

  return(list("ind_table" = ind_table, "f_table" = f_table))
  
}

tables <- fa_table(ARS_mod4_promax)
tables$ind_table
tables$f_table

```

#saving factor scores
```{r}
factorscores = factor.scores(ARS_items_only_rm1, ARS_mod4_promax)$scores

ARS_reg = ARS[, -c(1,8,11,14,16,25)]

ARS_reg = cbind(ARS_reg, factorscores)
View(ARS_reg)

ARS_reg <- ARS_reg %>% 	
  rename(AnimalResearch_concern = PA1,
         AnimalRights_concern = PA2)
summary(ARS_reg)

#running regressions
mod_reg <- lm(liberal ~ AnimalResearch_concern + AnimalRights_concern, data = ARS_reg)
summary(mod_reg)   

mod_reg1 <- lm(liberal ~ AnimalRights_concern + AnimalResearch_concern*AnimalRights_concern, data = ARS_reg)
summary(mod_reg1)


mod_mean <- lm(liberal ~ 1, data = ARS_reg)

anova(mod_mean, mod_reg)
anova(mod_reg, mod_reg1)
confint(mod_reg)
tab_model(mod_reg1)
```

